{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification for Pneumonia via X-rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "# This one uses CNN: https://www.kaggle.com/code/homayoonkhadivi/medical-diagnosis-with-cnn-transfer-learning\n",
    "# The codealong with CNN\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import pathlib\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.wrappers import scikit_learn\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data:\n",
    "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
    "\n",
    "Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. \n",
    "\n",
    "Due to the relatively small amount of validation data (16 images), I will create my own validation data instead of using it, but I will still load it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = 'chest_xray/train'\n",
    "testing = 'chest_xray/test'\n",
    "validation = 'chest_xray/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'chest_xray/train/NORMAL/'\n",
    "path = folder\n",
    "\n",
    "p = os.listdir(path)\n",
    "pf = pd.DataFrame(p)\n",
    "norm_weight = len(pf) / 5863\n",
    "\n",
    "print(f' There are {len(pf[0])} images in the training:normal folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'chest_xray/train/PNEUMONIA/'\n",
    "path = folder\n",
    "p = os.listdir(path)\n",
    "pf = pd.DataFrame(p)\n",
    "pneum_weight = len(pf) / 5863\n",
    "pf\n",
    "print(f' There are {len(pf[0])} images in the training:pneumonia folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Codealong CNN\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        'chest_xray/test', \n",
    "        target_size=(64, 64), batch_size = 624) \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        'chest_xray/val', \n",
    "        target_size=(64, 64), batch_size = 16)\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        'chest_xray/train', \n",
    "        target_size=(64, 64), batch_size=5216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(val_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.reshape(train_labels[:,0], (5216,1))\n",
    "test_y = np.reshape(test_labels[:,0], (624,1))\n",
    "val_y = np.reshape(val_labels[:,0], (16,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a baseline fully connected model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) # 2 hidden layers\n",
    "model.add(layers.Dense(7, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "baseline = model.fit(train_img,train_y,epochs=50, batch_size=32, \n",
    "                    validation_split= 0.1, steps_per_epoch= 25)\n",
    "\n",
    "\n",
    "train_loss = baseline.history['loss']\n",
    "train_acc = baseline.history['accuracy']\n",
    "val_loss = baseline.history['val_loss']\n",
    "val_acc = baseline.history['val_accuracy']\n",
    "\n",
    "#Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=baseline.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=baseline.epoch, y=train_acc, ax=ax2, label='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build function that builds the model so we can evaluate in sklearn\n",
    "def build_model():\n",
    "    model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) # 2 hidden layers\n",
    "    model.add(layers.Dense(7, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = scikit_learn.KerasClassifier(build_model,\n",
    "                                          epochs=50,\n",
    "                                          steps_per_epoch= 10,\n",
    "                                          batch_size=32,\n",
    "                                          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_img, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(test_img, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Confusion Matrix Code/ CF, as well as SaveResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))  \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=25,\n",
    "                    batch_size=32,  \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_loss = history.history['loss']\n",
    "sigmoid_acc = history.history['accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "sns.lineplot(x=history.epoch, y = sigmoid_loss, ax = ax1, label = 'loss')\n",
    "sns.lineplot(x=history.epoch, y = sigmoid_acc, ax = ax2, label = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN #1 Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64 ,64,  3)))  \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model2 = scikit_learn.KerasClassifier(build_cnn,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model2, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))  \n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model2.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model2.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_loss = history2.history['loss']\n",
    "sigmoid_acc = history2.history['accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "sns.lineplot(x=history2.epoch, y = sigmoid_loss, ax = ax1, label = 'loss')\n",
    "sns.lineplot(x=history2.epoch, y = sigmoid_acc, ax = ax2, label = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn2():\n",
    "    model2.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))  \n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model2.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model2.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model2.add(layers.Flatten())\n",
    "    model2.add(layers.Dense(64, activation='relu'))\n",
    "    model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model3 = scikit_learn.KerasClassifier(build_cnn2,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model3, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 3\n",
    "\n",
    "For this model, I will add batch normalization and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential()\n",
    "\n",
    "model3.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.15))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model3.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model3.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_loss = history3.history['loss']\n",
    "sigmoid_acc = history3.history['accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "sns.lineplot(x=history3.epoch, y = sigmoid_loss, ax = ax1, label = 'loss')\n",
    "sns.lineplot(x=history3.epoch, y = sigmoid_acc, ax = ax2, label = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn3():\n",
    "    model3.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Flatten())\n",
    "    model3.add(layers.Dense(64, activation='relu'))\n",
    "    model3.add(Dropout(0.15))\n",
    "    model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model3 = scikit_learn.KerasClassifier(build_cnn3,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model3, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 4\n",
    "\n",
    "For this model, I will change the optimizer to adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = models.Sequential()\n",
    "\n",
    "model4.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(64, activation='relu'))\n",
    "model4.add(Dropout(0.15))\n",
    "model4.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = model4.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model4.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model4.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_loss = history4.history['loss']\n",
    "sigmoid_acc = history4.history['accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "sns.lineplot(x=history4.epoch, y = sigmoid_loss, ax = ax1, label = 'loss')\n",
    "sns.lineplot(x=history4.epoch, y = sigmoid_acc, ax = ax2, label = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn4():\n",
    "    model4.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model4.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "    model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model4.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model4.add(layers.Flatten())\n",
    "    model4.add(layers.Dense(64, activation='relu'))\n",
    "    model4.add(Dropout(0.15))\n",
    "    model4.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model4.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model4 = scikit_learn.KerasClassifier(build_cnn4,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model4, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 5\n",
    "\n",
    "For this model, I will add weights and steps per epoch, with fewer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneum_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classWeight = {0:norm_weight, 1:pneum_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = models.Sequential()\n",
    "\n",
    "model5.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model5.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Flatten())\n",
    "model5.add(layers.Dense(64, activation='relu'))\n",
    "model5.add(Dropout(0.15))\n",
    "model5.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model5.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history5 = model5.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch= 100,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1,\n",
    "                    class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model5.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model5.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_loss = history5.history['loss']\n",
    "sigmoid_acc = history5.history['accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "sns.lineplot(x=history5.epoch, y = sigmoid_loss, ax = ax1, label = 'loss')\n",
    "sns.lineplot(x=history5.epoch, y = sigmoid_acc, ax = ax2, label = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn5():\n",
    "    model3.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "    model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Flatten())\n",
    "    model3.add(layers.Dense(64, activation='relu'))\n",
    "    model3.add(Dropout(0.15))\n",
    "    model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model5 = scikit_learn.KerasClassifier(build_cnn5,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2,\n",
    "                                            class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model4, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 6\n",
    "\n",
    "For this model, I will add dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = models.Sequential()\n",
    "\n",
    "model6.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Dropout(0.15))\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(Dropout(0.10))\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Flatten())\n",
    "model6.add(layers.Dense(64, activation='relu'))\n",
    "model6.add(Dropout(0.15))\n",
    "model6.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model6.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history6 = model6.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch= 100,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1,\n",
    "                    class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model6.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model6.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_loss = history6.history['loss']\n",
    "sigmoid_acc = history6.history['accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n",
    "sns.lineplot(x=history6.epoch, y = sigmoid_loss, ax = ax1, label = 'loss')\n",
    "sns.lineplot(x=history6.epoch, y = sigmoid_acc, ax = ax2, label = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn6():\n",
    "    model6.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model6.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model6.add(BatchNormalization())\n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "    model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model6.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model6.add(BatchNormalization())\n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model6.add(layers.Flatten())\n",
    "    model6.add(layers.Dense(64, activation='relu'))\n",
    "    model6.add(Dropout(0.15))\n",
    "    model6.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model6.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model6 = scikit_learn.KerasClassifier(build_cnn6,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2,\n",
    "                                            class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Re-splitting the data\n",
    "\n",
    "I am in an odd spot with this data - my validation scores are matching up with my training scores, but my testing scores are not. \n",
    "\n",
    "Because I did not split this data myself, I think there may be an issue with the split. So, I will add together all the data (the train and test sections provided) and re-split said data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('re-split_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = 're-split_data/NORMAL/'\n",
    "\n",
    "p = os.listdir(path)\n",
    "pf = pd.DataFrame(p)\n",
    "len(pf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 're-split_data/PNEUMONIA/'\n",
    "\n",
    "p = os.listdir(path)\n",
    "pf = pd.DataFrame(p)\n",
    "len(pf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 're-split_data/NORMAL'\n",
    "y = 're-split_data/PNEUMONIA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Codealong CNN\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "X_generator2 = ImageDataGenerator().flow_from_directory('re-split_data/NORMAL', target_size=(64, 64), batch_size = 1576) \n",
    "\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "y_generator2 = ImageDataGenerator().flow_from_directory('re-split_data/PNEUMONIA', target_size=(64, 64), batch_size=4221)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(val_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.reshape(train_labels[:,0], (5216,1))\n",
    "test_y = np.reshape(test_labels[:,0], (624,1))\n",
    "val_y = np.reshape(val_labels[:,0], (16,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6405dd8549f94b84e26eaa9f1d89f5de150e988f9022df8484d4567d5362723e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
