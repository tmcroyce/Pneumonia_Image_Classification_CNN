{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification for Pneumonia -- MY SPLIT VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "# This one uses CNN: https://www.kaggle.com/code/homayoonkhadivi/medical-diagnosis-with-cnn-transfer-learning\n",
    "# The codealong with CNN\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import pathlib\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.wrappers import scikit_learn\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data:\n",
    "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
    "\n",
    "Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. \n",
    "\n",
    "Due to the relatively small amount of validation data (16 images), I will create my own validation data instead of using it, but I will still load it in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Personal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: The origin of this confusion matrix code was found on medium, '\n",
    "# from https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            a = cf[0,0]\n",
    "            b = cf[0,1]\n",
    "            c = cf[1,0]\n",
    "            d = cf[1,1]\n",
    "            tn = ((a / (a+b))*100).round(2).astype(str) + '%'\n",
    "            fp = ((b / (a+b))*100).round(2).astype(str) + '%'\n",
    "            fn = ((c / (c+d))*100).round(2).astype(str) + '%'\n",
    "            tp = ((d / (c+d))*100).round(2).astype(str) + '%'\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            rwf_score = 2*precision* (recall*2) /(precision + (recall*2))\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages =  [tn,fp,fn,tp]\n",
    "        # old = group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label', weight = 'bold')\n",
    "        plt.xlabel('Predicted label' + stats_text, weight = 'bold')\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title,size = 20, weight = 'bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Result Saving Initial Function\n",
    "dfcols = ['model_name', 'Train Accuracy', 'Test Accuracy']\n",
    "model_summary = pd.DataFrame(columns=dfcols)\n",
    "\n",
    "\n",
    "def save_result(model_name, Train_Accuracy, Test_Accuracy):\n",
    "            global model_summary\n",
    "            row = [(model_name, Train_Accuracy, Test_Accuracy)]\n",
    "            res = pd.DataFrame(columns = dfcols, data = row)\n",
    "            yeep = [model_summary, res]\n",
    "            model_summary = pd.concat(yeep)\n",
    "            model_summary = model_summary.sort_values('Test Accuracy', ascending = False)\n",
    "            model_summary = model_summary.drop_duplicates()\n",
    "            return model_summary.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 1575 images in the normal folder\n"
     ]
    }
   ],
   "source": [
    "folder = 're-split_data/NORMAL'\n",
    "path = folder\n",
    "\n",
    "p = os.listdir(path)\n",
    "pf = pd.DataFrame(p)\n",
    "\n",
    "norm_tot = len(pf)\n",
    "print(f' There are {len(pf[0])} images in the normal folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 4265 images in the pneumonia folder\n",
      " there are 5841 total images in the dataset\n"
     ]
    }
   ],
   "source": [
    "folder = 're-split_data/PNEUMONIA'\n",
    "path = folder\n",
    "p = os.listdir(path)\n",
    "pf = pd.DataFrame(p)\n",
    "pneum_tot = len(pf)\n",
    "pneum_weight = len(pf) / 5863\n",
    "norm_weight = 1 - pneum_weight\n",
    "pf\n",
    "print(f' There are {len(pf[0])} images in the pneumonia folder')\n",
    "print(f' there are {1576 + len(pf[0])} total images in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The weight of pneumonia is 0.7274432884188982\n",
      " The weight of normal is 0.27255671158110184\n"
     ]
    }
   ],
   "source": [
    "print(f' The weight of pneumonia is {pneum_weight}')\n",
    "print(f' The weight of normal is {norm_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, there is some code which I used to re-split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnorm_train = norm_tot * .7\\nnorm_test = norm_tot * .3\\npneum_train = pneum_tot * .7\\npneum_test = pneum_tot * .3\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define test and train split percentages\n",
    "'''\n",
    "norm_train = norm_tot * .7\n",
    "norm_test = norm_tot * .3\n",
    "pneum_train = pneum_tot * .7\n",
    "pneum_test = pneum_tot * .3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npf = os.listdir('re-split_data/NORMAL')\\nrand_norm_files = random.sample(pf, int(norm_train))\\nfor file in rand_norm_files:\\n    shutil.copy('re-split_data/NORMAL/' + file, 're-split_data/train/normal')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pf = os.listdir('re-split_data/NORMAL')\n",
    "rand_norm_files = random.sample(pf, int(norm_train))\n",
    "for file in rand_norm_files:\n",
    "    shutil.copy('re-split_data/NORMAL/' + file, 're-split_data/train/normal')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\np1 = os.listdir('re-split_data/train/normal')\\np1 = pd.DataFrame(p1)\\n\\np2 = os.listdir('re-split_data/NORMAL')\\np2 = pd.DataFrame(p2)\\n\\ntester_files = pd.concat([p1[0],p2[0]]).drop_duplicates(keep=False)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "p1 = os.listdir('re-split_data/train/normal')\n",
    "p1 = pd.DataFrame(p1)\n",
    "\n",
    "p2 = os.listdir('re-split_data/NORMAL')\n",
    "p2 = pd.DataFrame(p2)\n",
    "\n",
    "tester_files = pd.concat([p1[0],p2[0]]).drop_duplicates(keep=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in tester_files:\n",
    "    #shutil.copy('re-split_data/NORMAL/' + file, 're-split_data/test/normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# The pneumonia files\\npf = os.listdir('re-split_data/PNEUMONIA')\\n\\nrand_Pfiles = random.sample(pf, int(pneum_train))\\n\\nfor file in rand_Pfiles:\\n    shutil.copy('re-split_data/PNEUMONIA/' + file, 're-split_data/train/pneumonia')\\n\\np3 = os.listdir('re-split_data/train/pneumonia')\\np3 = pd.DataFrame(p3)\\n\\np4 = os.listdir('re-split_data/PNEUMONIA')\\np4 = pd.DataFrame(p4)\\n\\ntester_p =  pd.concat([p3[0],p4[0]]).drop_duplicates(keep=False)\\n\\nfor file in tester_p:\\n    shutil.copy('re-split_data/PNEUMONIA/' + file, 're-split_data/test/pneumonia')\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The pneumonia files\n",
    "pf = os.listdir('re-split_data/PNEUMONIA')\n",
    "\n",
    "rand_Pfiles = random.sample(pf, int(pneum_train))\n",
    "\n",
    "for file in rand_Pfiles:\n",
    "    shutil.copy('re-split_data/PNEUMONIA/' + file, 're-split_data/train/pneumonia')\n",
    "\n",
    "p3 = os.listdir('re-split_data/train/pneumonia')\n",
    "p3 = pd.DataFrame(p3)\n",
    "\n",
    "p4 = os.listdir('re-split_data/PNEUMONIA')\n",
    "p4 = pd.DataFrame(p4)\n",
    "\n",
    "tester_p =  pd.concat([p3[0],p4[0]]).drop_duplicates(keep=False)\n",
    "\n",
    "for file in tester_p:\n",
    "    shutil.copy('re-split_data/PNEUMONIA/' + file, 're-split_data/test/pneumonia')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 4088 images in the training set, 1752 in the test set\n"
     ]
    }
   ],
   "source": [
    "train_size = len(os.listdir('re-split_data/train/normal')) + len(os.listdir('re-split_data/train/pneumonia'))\n",
    "test_size = len(os.listdir('re-split_data/test/normal')) + len(os.listdir('re-split_data/test/pneumonia'))\n",
    "print(f' There are {train_size} images in the training set, {test_size} in the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test and Train Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4088 images belonging to 2 classes.\n",
      "Found 1752 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get all the data in the directory split/test , and reshape them\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory('re-split_data/train',\n",
    "        target_size=(64, 64), batch_size = 4089) \n",
    "\n",
    "test_generator =ImageDataGenerator(rescale=1./255).flow_from_directory('re-split_data/test',\n",
    "        target_size=(64, 64), batch_size = 1753) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.reshape(train_labels[:,0], (4088,1))\n",
    "test_y = np.reshape(test_labels[:,0], (1752,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a baseline fully connected model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) # 2 hidden layers\n",
    "model.add(layers.Dense(7, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "baseline = model.fit(train_img,train_y,epochs=50, batch_size=32, \n",
    "                    validation_split= 0.1, steps_per_epoch= 25)\n",
    "\n",
    "\n",
    "train_loss = baseline.history['loss']\n",
    "train_acc = baseline.history['accuracy']\n",
    "val_loss = baseline.history['val_loss']\n",
    "val_acc = baseline.history['val_accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=baseline.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=baseline.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
    "sns.lineplot(x=baseline.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
    "sns.lineplot(x=baseline.epoch, y=val_acc, ax=ax2, label='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build function that builds the model so we can evaluate in sklearn\n",
    "def build_model():\n",
    "    model.add(layers.Dense(20, activation='relu', input_shape=(12288,))) # 2 hidden layers\n",
    "    model.add(layers.Dense(7, activation='relu'))\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='sgd',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = scikit_learn.KerasClassifier(build_model,\n",
    "                                          epochs=50,\n",
    "                                          steps_per_epoch= 10,\n",
    "                                          batch_size=32,\n",
    "                                          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_img, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(test_img, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('Initial Model', results_train[1], results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Confusion Matrix Code/ CF, as well as SaveResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))  \n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch=25,\n",
    "                    batch_size=32,  \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('CNN #1', results_train[1], results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=history.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
    "sns.lineplot(x=history.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
    "sns.lineplot(x=history.epoch, y=val_acc, ax=ax2, label='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN #1 Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64 ,64,  3)))  \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model2 = scikit_learn.KerasClassifier(build_cnn,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model2, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))  \n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model2.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model2.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('CNN #2', results_train[1], results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history2.history['loss']\n",
    "train_acc = history2.history['accuracy']\n",
    "val_loss = history2.history['val_loss']\n",
    "val_acc = history2.history['val_accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history2.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=history2.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
    "sns.lineplot(x=history2.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
    "sns.lineplot(x=history2.epoch, y=val_acc, ax=ax2, label='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn2():\n",
    "    model2 = models.Sequential()\n",
    "    model2.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64 ,64,  3)))  \n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model2.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model2.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model2.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model2.add(layers.Flatten())\n",
    "    model2.add(layers.Dense(64, activation='relu'))\n",
    "    model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model3 = scikit_learn.KerasClassifier(build_cnn2,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model3, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 3\n",
    "\n",
    "For this model, I will add batch normalization and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = models.Sequential()\n",
    "\n",
    "model3.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model3.add(layers.Flatten())\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.15))\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model3.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model3.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('CNN #3', results_train[1], results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history3.history['loss']\n",
    "train_acc = history3.history['accuracy']\n",
    "val_loss = history3.history['val_loss']\n",
    "val_acc = history3.history['val_accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history3.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=history3.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
    "sns.lineplot(x=history3.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
    "sns.lineplot(x=history3.epoch, y=val_acc, ax=ax2, label='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn3():\n",
    "    model3 = models.Sequential()\n",
    "    model3.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "    model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Flatten())\n",
    "    model3.add(layers.Dense(64, activation='relu'))\n",
    "    model3.add(Dropout(0.15))\n",
    "    model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model3 = scikit_learn.KerasClassifier(build_cnn3,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model3, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 4\n",
    "\n",
    "For this model, I will change the optimizer to adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = models.Sequential()\n",
    "\n",
    "model4.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model4.add(layers.Flatten())\n",
    "model4.add(layers.Dense(64, activation='relu'))\n",
    "model4.add(Dropout(0.15))\n",
    "model4.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = model4.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model4.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model4.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('CNN #4', results_train[1], results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history4.history['loss']\n",
    "train_acc = history4.history['accuracy']\n",
    "val_loss = history4.history['val_loss']\n",
    "val_acc = history4.history['val_accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history4.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=history4.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
    "sns.lineplot(x=history4.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
    "sns.lineplot(x=history4.epoch, y=val_acc, ax=ax2, label='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn4():\n",
    "    model4 = models.Sequential()\n",
    "    model4.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model4.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "    model4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model4.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model4.add(BatchNormalization())\n",
    "    model4.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model4.add(layers.Flatten())\n",
    "    model4.add(layers.Dense(64, activation='relu'))\n",
    "    model4.add(Dropout(0.15))\n",
    "    model4.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model4.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model4 = scikit_learn.KerasClassifier(build_cnn4,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model4, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 5\n",
    "\n",
    "For this model, I will add weights and steps per epoch, with fewer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneum_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classWeight = {0:norm_weight, 1:pneum_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = models.Sequential()\n",
    "\n",
    "model5.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model5.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model5.add(layers.Flatten())\n",
    "model5.add(layers.Dense(64, activation='relu'))\n",
    "model5.add(Dropout(0.15))\n",
    "model5.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model5.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history5 = model5.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch= 100,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1,\n",
    "                    class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model5.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model5.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('CNN #5', results_train[1], results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history5.history['loss']\n",
    "train_acc = history5.history['accuracy']\n",
    "val_loss = history5.history['val_loss']\n",
    "val_acc = history5.history['val_accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history5.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=history5.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
    "sns.lineplot(x=history5.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
    "sns.lineplot(x=history5.epoch, y=val_acc, ax=ax2, label='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn5():\n",
    "    model3 = models.Sequential()\n",
    "    model3.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "    model3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model3.add(BatchNormalization())\n",
    "    model3.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model3.add(layers.Flatten())\n",
    "    model3.add(layers.Dense(64, activation='relu'))\n",
    "    model3.add(Dropout(0.15))\n",
    "    model3.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "    model3.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model5 = scikit_learn.KerasClassifier(build_cnn5,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2,\n",
    "                                            class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that it is a keras model, you can cross-validate it\n",
    "cross_val_score(keras_model4, train_img, train_y, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model 6\n",
    "\n",
    "For this model, I will add dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = models.Sequential()\n",
    "\n",
    "model6.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Dropout(0.15))\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(Dropout(0.10))\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model6.add(layers.Flatten())\n",
    "model6.add(layers.Dense(64, activation='relu'))\n",
    "model6.add(Dropout(0.15))\n",
    "model6.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model6.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history6 = model6.fit(train_images,\n",
    "                    train_y,\n",
    "                    epochs=50,\n",
    "                    steps_per_epoch= 100,\n",
    "                    batch_size=32,\n",
    "                    validation_split= .1,\n",
    "                    class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model6.evaluate(train_images, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model6.evaluate(test_images, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result('CNN #6', results_train[1], results_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history6.history['loss']\n",
    "train_acc = history6.history['accuracy']\n",
    "val_loss = history6.history['val_loss']\n",
    "val_acc = history6.history['val_accuracy']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=history6.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
    "sns.lineplot(x=history6.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
    "sns.lineplot(x=history6.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
    "sns.lineplot(x=history6.epoch, y=val_acc, ax=ax2, label='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn6():\n",
    "    model6.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(64 ,64,  3)))  \n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model6.add(layers.Conv2D(32, (4, 4), activation='relu'))\n",
    "    model6.add(BatchNormalization())\n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "    model6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model6.add(layers.Conv2D(96, (3, 3), activation='relu'))\n",
    "    model6.add(BatchNormalization())\n",
    "    model6.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model6.add(layers.Flatten())\n",
    "    model6.add(layers.Dense(64, activation='relu'))\n",
    "    model6.add(Dropout(0.15))\n",
    "    model6.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model6.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model6 = scikit_learn.KerasClassifier(build_cnn6,\n",
    "                                            epochs=50,\n",
    "                                            steps_per_epoch=25,\n",
    "                                            validation_split= .1,\n",
    "                                            batch_size=32,\n",
    "                                            verbose=2,\n",
    "                                            class_weight=classWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('learn-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95aea3fdbc92d546f3589630df01593dce236e625eaca868c57675f7e41ddd2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
